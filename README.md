# Code Snippet Analysis with Transformer Models

This project analyzes a set of Python code snippets by combining static code analysis with modern NLP techniques using pre-trained transformer models. It demonstrates how different models, such as **MiniLM**, **DistilRoBERTa**, and **MPNet**, represent code in their embedding spaces.

## Project Overview

The project workflow is as follows:

1.  **Code Snippet Preparation**: A collection of 10 diverse Python code snippets is created, covering functions, classes, loops, and error handling.
2.  **Static Analysis with AST**: Python's built-in `ast` module is used to parse each snippet into an Abstract Syntax Tree. Key elements like function names, class names, imports, and code patterns (e.g., `for` loops, `if` statements, `try-except` blocks) are extracted.
3.  **Tokenization**: Each code snippet is tokenized using the tokenizer specific to each of the three pre-trained models. This ensures the input is in a format the models can understand.
4.  **Model Processing**: The tokenized snippets are passed through the MiniLM, DistilRoBERTa, and MPNet models to obtain vector representations (embeddings) of the code.
5.  **Embedding Comparison**: The embeddings from the different models are compared. Since the models have different embedding dimensions (MiniLM: 384, others: 768), Principal Component Analysis (PCA) is applied to reduce all embeddings to a common dimension of 10. Cosine similarity is then calculated to quantify the similarity between the embeddings generated by each model pair.
6.  **Visualization**: A scatter plot is generated using the first two PCA components to visually represent the relationships between the code embeddings from the different models.
7.  **Summary**: The results from the numerical comparison and the visualization are summarized to draw conclusions about the similarities and differences in how each model interprets the code.

## Key Findings

### Data Analysis

  * **Static Analysis**: The `ast` module successfully identified structural components of the code, providing a foundation for understanding each snippet's purpose.
  * **Dimensionality Mismatch**: A key challenge was the difference in embedding dimensions across the models. This was addressed by using PCA for dimensionality reduction, which allowed for a direct comparison.
  * **Model Similarity**: After reducing the embeddings to 10 dimensions, the analysis revealed the following average cosine similarities:
      * `DistilRoBERTa_vs_MPNet`: 0.0358
      * `MiniLM_vs_MPNet`: -0.0468
      * `MiniLM_vs_DistilRoBERta`: -0.3178
  * **Visual Clustering**: The PCA scatter plot graphically confirmed the numerical results. Embeddings from the same model tended to cluster together, with MPNet forming a particularly distinct group. DistilRoBERTa and MiniLM embeddings were more intermingled, but still separable, suggesting they capture some similar semantic properties, albeit with notable differences.

### Insights and Next Steps

  * **Model Representations**: The results suggest that **DistilRoBERTa** and **MPNet** produce more similar code representations to each other than to **MiniLM**. This could be due to architectural similarities or shared training data characteristics. MiniLM seems to generate a more unique representation for the given dataset.
  * **Limitations**: This analysis is based on a very small sample size of just 10 code snippets. To draw more robust and generalizable conclusions, a much larger and more diverse dataset would be required.
  * **Future Work**: Future explorations could involve:
      * Using a larger dataset of code snippets from different languages and domains.
      * Implementing a dedicated projection layer (e.g., a simple feed-forward neural network) instead of PCA to align the embedding spaces more effectively.
      * Analyzing the embeddings' ability to perform downstream tasks like code classification or similarity search.

## How to Run the Code

The analysis was performed in a Jupyter Notebook. To replicate this project, you'll need a Python environment with the following libraries:

```bash
pip install transformers torch scikit-learn pandas matplotlib seaborn
```

The core logic is contained within the notebook cells, which you can run sequentially. The code snippets, analysis steps, and visualizations are all generated programmatically within the notebook.
